{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpE3ES1u6vNH",
        "outputId": "c70926ac-0444-4dcd-f96e-377f3130423f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, rank_bm25, fsspec, dill, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.12.0 multiprocess-0.70.16 rank_bm25-0.2.2 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install transformers rank_bm25 datasets evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from rank_bm25 import BM25Okapi\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª SQuAD (Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©)\n",
        "dataset = load_dataset(\"squad\")\n",
        "questions = [q[\"question\"] for q in dataset[\"train\"]]\n",
        "contexts = [q[\"context\"] for q in dataset[\"train\"]]\n",
        "answers = [q[\"answers\"][\"text\"][0] for q in dataset[\"train\"]]\n",
        "\n",
        "# Ù†Ø³ØªØ®Ø¯Ù… ÙÙ‚Ø· 1000 Ù…Ø«Ø§Ù„ Ù„Ù„Ø³Ø±Ø¹Ø©\n",
        "questions = questions[:1000]\n",
        "contexts = contexts[:1000]\n",
        "answers = answers[:1000]\n",
        "\n",
        "# Tokenization Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©\n",
        "tokenized_corpus = [context.split() for context in contexts]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "# Ø¯Ø§Ù„Ø© Ø§Ù„Ø¨Ø­Ø«\n",
        "def classical_ir_search(query, n=5):\n",
        "    tokenized_query = query.split()\n",
        "    doc_scores = bm25.get_scores(tokenized_query)\n",
        "    top_n = sorted(range(len(doc_scores)), key=lambda i: doc_scores[i], reverse=True)[:n]\n",
        "    return [contexts[i] for i in top_n]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDE8HOiF7CfC",
        "outputId": "e703b4bb-7bb3-43ec-99b9-1eb9939724cb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ BERT Ù„Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©\n",
        "model_name = \"distilbert-base-cased-distilled-squad\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# Ø¯Ø§Ù„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬ÙˆØ§Ø¨ Ø¨ÙˆØ§Ø³Ø·Ø© LLM\n",
        "def llm_answer(question, context):\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    answer_start = torch.argmax(outputs.start_logits)\n",
        "    answer_end = torch.argmax(outputs.end_logits) + 1\n",
        "    answer = tokenizer.convert_tokens_to_string(\n",
        "        tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end])\n",
        "    )\n",
        "    return answer\n",
        "\n",
        "# Ø¯Ø§Ù„Ø© ÙƒØ§Ù…Ù„Ø©: Ø³Ø¤Ø§Ù„ â¡ï¸ Ø¨Ø­Ø« â¡ï¸ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ â¡ï¸ Ø¬ÙˆØ§Ø¨\n",
        "def qa_system(question):\n",
        "    retrieved_contexts = classical_ir_search(question, n=1)\n",
        "    best_context = retrieved_contexts[0]\n",
        "    answer = llm_answer(question, best_context)\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "iHfvCVXg8GX3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert_score\n"
      ],
      "metadata": {
        "id": "DJucGhOdI2jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "bertscore = load(\"bertscore\")\n",
        "\n",
        "def evaluate_methods(question, true_answer):\n",
        "    # Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠØ©\n",
        "    bm25_results = classical_ir_search(question)\n",
        "    bm25_answer = bm25_results[0][:500]  # Ù†Ø£Ø®Ø° Ø£ÙˆÙ„ 500 Ø­Ø±Ù Ù…Ù† Ø§Ù„ÙÙ‚Ø±Ø©\n",
        "\n",
        "    # Ø§Ù„Ø¨Ø­Ø« Ø¨Ù†Ù…ÙˆØ°Ø¬ LLM\n",
        "    llm_context = bm25_results[0]  # Ù†Ø³ØªØ®Ø¯Ù… Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø© Ù…Ù† BM25 ÙƒØ³ÙŠØ§Ù‚\n",
        "    predicted_answer = llm_answer(question, llm_context)\n",
        "\n",
        "    # Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BERTScore Ù…Ø¹ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©\n",
        "    bm25_score = bertscore.compute(\n",
        "        predictions=[bm25_answer], references=[true_answer], lang=\"en\"\n",
        "    )[\"f1\"][0]\n",
        "\n",
        "    llm_score = bertscore.compute(\n",
        "        predictions=[predicted_answer], references=[true_answer], lang=\"en\"\n",
        "    )[\"f1\"][0]\n",
        "\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"true_answer\": true_answer,\n",
        "        \"bm25_answer\": bm25_answer,\n",
        "        \"llm_answer\": predicted_answer,\n",
        "        \"bm25_score\": bm25_score,\n",
        "        \"llm_score\": llm_score\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Xvi1UX7vF3Ap"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø¹Ø´ÙˆØ§Ø¦ÙŠ ---\n",
        "sample_idx = 42\n",
        "\n",
        "result = evaluate_methods(questions[sample_idx], answers[sample_idx])\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"ğŸ”¹ Ø§Ù„Ø³Ø¤Ø§Ù„:\\n{result['question']}\")\n",
        "print(f\"\\nâœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©:\\n{result['true_answer']}\")\n",
        "print(f\"\\nğŸ“„ Ø¥Ø¬Ø§Ø¨Ø© BM25:\\n{result['bm25_answer']}\\n(Ø§Ù„ØªÙ‚ÙŠÙŠÙ…: {result['bm25_score']:.2f})\")\n",
        "print(f\"\\nğŸ¤– Ø¥Ø¬Ø§Ø¨Ø© BERT:\\n{result['llm_answer']}\\n(Ø§Ù„ØªÙ‚ÙŠÙŠÙ…: {result['llm_score']:.2f})\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# --- ØªÙ‚ÙŠÙŠÙ… Ø¹Ù„Ù‰ Ø¹Ø¯Ø© Ø£Ø³Ø¦Ù„Ø© ---\n",
        "import numpy as np\n",
        "\n",
        "scores = []\n",
        "\n",
        "# Ù†Ø¬Ø±Ø¨ Ø¹Ù„Ù‰ Ø£ÙˆÙ„ 50 Ø³Ø¤Ø§Ù„ Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„ØªÙ†ÙÙŠØ°\n",
        "for i in range(100):\n",
        "    res = evaluate_methods(questions[i], answers[i])\n",
        "    scores.append((res[\"bm25_score\"], res[\"llm_score\"]))\n",
        "\n",
        "# Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª\n",
        "bm25_avg = np.mean([s[0] for s in scores])\n",
        "llm_avg = np.mean([s[1] for s in scores])\n",
        "\n",
        "print(\"\\nğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¹Ù„Ù‰ 100 Ø³Ø¤Ø§Ù„:\")\n",
        "print(f\"ğŸ”µ Ù…ØªÙˆØ³Ø· ØªÙ‚ÙŠÙŠÙ… BM25: {bm25_avg:.2f}\")\n",
        "print(f\"ğŸŸ¢ Ù…ØªÙˆØ³Ø· ØªÙ‚ÙŠÙŠÙ… BERT QA: {llm_avg:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQcfqRFuHYlv",
        "outputId": "b3126fa2-4197-43c7-9ff1-dc0e5fd7553e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "ğŸ”¹ Ø§Ù„Ø³Ø¤Ø§Ù„:\n",
            "What percentage of students at Notre Dame participated in the Early Action program?\n",
            "\n",
            "âœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©:\n",
            "39.1%\n",
            "\n",
            "ğŸ“„ Ø¥Ø¬Ø§Ø¨Ø© BM25:\n",
            "In 2015-2016, Notre Dame ranked 18th overall among \"national universities\" in the United States in U.S. News & World Report's Best Colleges 2016. In 2014, USA Today ranked Notre Dame 10th overall for American universities based on data from College Factual. Forbes.com's America's Best Colleges ranks Notre Dame 13th among colleges in the United States in 2015, 8th among Research Universities, and 1st in the Midwest. U.S. News & World Report also lists Notre Dame Law School as 22nd overall. Busine\n",
            "(Ø§Ù„ØªÙ‚ÙŠÙŠÙ…: 0.81)\n",
            "\n",
            "ğŸ¤– Ø¥Ø¬Ø§Ø¨Ø© BERT:\n",
            "57. 6 %\n",
            "(Ø§Ù„ØªÙ‚ÙŠÙŠÙ…: 0.96)\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¹Ù„Ù‰ 50 Ø³Ø¤Ø§Ù„:\n",
            "ğŸ”µ Ù…ØªÙˆØ³Ø· ØªÙ‚ÙŠÙŠÙ… BM25: 0.80\n",
            "ğŸŸ¢ Ù…ØªÙˆØ³Ø· ØªÙ‚ÙŠÙŠÙ… BERT QA: 0.91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PlQWITFNJrEl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}